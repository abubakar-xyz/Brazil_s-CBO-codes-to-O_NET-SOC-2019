# -*- coding: utf-8 -*-
"""CBO to O*NET-SOC 2019 Crosswalk Generation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RFWNcNIyPSORwQ06IWGByQp88UhQUc1w
"""

import pandas as pd
import numpy as np
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# --- Configuration ---
CBO_PATH = 'cbo-isco-conc.csv'
ISCO_SOC_PATH = 'isco_soc_crosswalk - ISCO-08 to 2010 SOC.csv'
SOC2010_2018_PATH = 'soc_2010_to_2018_crosswalk.xlsx'
CROSS2019_PATH = '2019_to_SOC_Crosswalk.csv'
OCC_DATA_PATH = 'Occupation Data.xlsx'

# List to collect stats
mapping_stats = []

# Helper functions
def clean_codes(df, cols, zfill_map=None):
    """
    Cleans and standardizes code columns:
      - strips whitespace
      - ensures string dtype
      - replaces empty strings with NaN
      - optionally zero-fills specified columns
    """
    df = df.copy()
    for col in cols:
        df[col] = df[col].astype(str).str.strip()
    df.replace('', np.nan, inplace=True)
    if zfill_map:
        for col, width in zfill_map.items():
            df[col] = df[col].str.zfill(width)
    return df

# --- 1. Load Data ---
logger.info('Stage 1: Loading data files')
cbo_isco = pd.read_csv(CBO_PATH, dtype=str)
isco_soc = pd.read_csv(ISCO_SOC_PATH, dtype=str)
soc2010_to_2018 = pd.read_excel(SOC2010_2018_PATH, header=8, dtype=str)
cross2019 = pd.read_csv(CROSS2019_PATH, dtype=str)
occ_data = pd.read_excel(OCC_DATA_PATH, dtype=str)
logger.info('Data loading complete')

# --- Stage 1: CBO → ISCO-08 ---
logger.info('Stage 2: Cleaning CBO→ISCO mapping')
initial_cbo = len(cbo_isco)
# Retain only columns and clean
df = cbo_isco[['cboid','iscoid']].copy()
df = clean_codes(df, ['cboid','iscoid'], zfill_map={'iscoid':4})
df = df.rename(columns={'iscoid':'ISCO08'})
# Indicator of mapping
df['has_ISCO08'] = df['ISCO08'].notna()
mapping_stats.append(f"CBO→ISCO08: {df['has_ISCO08'].sum()}/{initial_cbo} mapped")
logger.info(mapping_stats[-1])

# --- Stage 2: ISCO-08 → SOC2010 ---
logger.info('Stage 3: Merging ISCO-08→SOC2010')
prev = len(df)
isco_soc = isco_soc.rename(columns={'ISCO-08 Code':'ISCO08','2010 SOC Code':'SOC2010'})
isco_soc = clean_codes(isco_soc, ['ISCO08','SOC2010'], zfill_map={'ISCO08':4})
merged = df.merge(
    isco_soc[['ISCO08','SOC2010']],
    on='ISCO08',
    how='left',
    validate='many_to_many'
)
merged['has_SOC2010'] = merged['SOC2010'].notna()
mapping_stats.append(f"ISCO08→SOC2010: {merged['has_SOC2010'].sum()}/{prev} mapped")
logger.info(mapping_stats[-1])

# --- Stage 3: SOC2010 → SOC2018 ---
logger.info('Stage 4: Merging SOC2010→SOC2018')
prev = len(merged)
soc2010_to_2018 = soc2010_to_2018.rename(columns={'2010 SOC Code':'SOC2010','2018 SOC Code':'SOC2018'})
soc2010_to_2018 = clean_codes(soc2010_to_2018, ['SOC2010','SOC2018'])
merged = merged.merge(
    soc2010_to_2018[['SOC2010','SOC2018']],
    on='SOC2010',
    how='left',
    validate='many_to_many'
)
merged['has_SOC2018'] = merged['SOC2018'].notna()
mapping_stats.append(f"SOC2010→SOC2018: {merged['has_SOC2018'].sum()}/{prev} mapped")
logger.info(mapping_stats[-1])

# --- Stage 4: SOC2018 → O*NET-SOC 2019 ---
logger.info('Stage 5: Merging SOC2018→O*NET2019')
prev = len(merged)
cross2019 = cross2019.rename(columns={'2018 SOC Code':'SOC2018','O*NET-SOC 2019 Code':'ONET2019'})
cross2019 = clean_codes(cross2019, ['SOC2018','ONET2019'])
merged = merged.merge(
    cross2019[['SOC2018','ONET2019']],
    on='SOC2018',
    how='left',
    validate='many_to_many'
)
merged['has_ONET2019'] = merged['ONET2019'].notna()
mapping_stats.append(f"SOC2018→O*NET2019: {merged['has_ONET2019'].sum()}/{prev} mapped")
logger.info(mapping_stats[-1])

# --- Stage 5: Attach O*NET Title/Description ---
logger.info('Stage 6: Attaching Title/Description')
prev = len(merged)
occ_data = occ_data.rename(columns={'O*NET-SOC Code':'ONET2019'})
occ_data = clean_codes(occ_data, ['ONET2019','Title','Description'])
merged = merged.merge(
    occ_data[['ONET2019','Title','Description']],
    on='ONET2019',
    how='left',
    validate='many_to_many'
)
merged['has_TitleDesc'] = merged['Title'].notna() & merged['Description'].notna()
mapping_stats.append(f"Attach Title/Desc: {merged['has_TitleDesc'].sum()}/{prev} mapped")
logger.info(mapping_stats[-1])

# --- Final Assembly ---
logger.info('Stage 7: Finalizing crosswalk')
# Drop rows without full mapping path if desired
full = merged[
    merged[['has_ISCO08','has_SOC2010','has_SOC2018','has_ONET2019','has_TitleDesc']].all(axis=1)
].copy()
# Rename final column and reorder
full = full.rename(columns={'ONET2019':'O*NET-SOC 2019 Code'})
cols = ['cboid','ISCO08','SOC2010','SOC2018','O*NET-SOC 2019 Code','Title','Description']
full = full[cols]

# Schema assertions
assert full['ISCO08'].str.match(r'^\d{4}$').all(), "Invalid ISCO08 codes"
assert full['SOC2010'].str.match(r'^\d{2}-\d{4}$').all(), "Invalid SOC2010 codes"
assert full['SOC2018'].str.match(r'^\d{2}-\d{4}$').all(), "Invalid SOC2018 codes"
assert full['O*NET-SOC 2019 Code'].str.match(r'^\d{2}-\d{4}\.\d{2}$').all(), "Invalid O*NET-SOC 2019 codes"

# Export
csv_out = 'CBO_ISCO_SOC_ONET_Crosswalk_Cleaned.csv'
json_out = 'CBO_ISCO_SOC_ONET_Crosswalk_Cleaned.json'
full.to_csv(csv_out, index=False, encoding='utf-8-sig')
full.to_json(json_out, orient='records', force_ascii=False, indent=4)

logger.info('Exported final crosswalk')
logger.info('Mapping summary:')
for s in mapping_stats:
    logger.info('  - %s', s)